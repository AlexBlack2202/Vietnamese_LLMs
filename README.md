
<h1 align="center">
  <span> üáªüá≥ C·ªông ƒë·ªìng LLMs Vi·ªát Nam - Vietnamese Language Models Community</span>
</h1>
<!--
<div align="center">
     <img width="auto" height="400px" src="./images/Vietnamese_LLMs.png"/>
</div>
-->

## üí° Get help - [Q&A](https://github.com/TranNhiem/Vietnamese_LLMs/discussions) or [Discord üí¨](https://discord.gg/BC8Mqq8qYn)

# News: 
+ [2023.09.02] We release LLaMA2 7B, 13B (8k Context Length 200k)fine-tuning on 200k Vietnamese Mix Instruction üî•
+ [2023.07.28] We release LLaMA 13B, 30B (2k Context Length) on 52k Vietnamese alpaca and 200k Mix Instruction Dataset üî•
+ [2023.08.27] We release BLOOMZ 1.7B, 7B instruction fine-tuning on 52k Vietnamese alpacaüî•üî•


We provide a number of model checkpoints that we trained. Please find them on Hugging Face [here](https://huggingface.co/models?search=taiwan-llama). Here are some quick links to the checkpoints that are finetuned from LLaMa 2:

| **Model**         |                   **Link**                                                            | 
|--------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Vietnamese-LLaMa2 v1.0 13B 8K Context Length**  | ü§ó <a href="https://huggingface.co/VietnamAIHub/Vietnamese_LLama2_13B_8K_SFT_General_Domain_Knowledge" target="_blank">Vietnamese_LLama2_13B_8K_SFT_General_Domain_Knowledge</a>  | 
| **Vietnamese-LLaMa2 v1.0 7B 8K Context Length**  | ü§ó <a href="https://huggingface.co/VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain" target="_blank">Vietnamese_llama2_7B_8K_SFT_General_domain</a>  | 
| **Vietnamese-LLaMa v1.0 30B 2K Context Length** | ü§ó <a href="https://huggingface.co/VietnamAIHub/Vietnamese_llama_30B_SFT" target="_blank">Vietnamese_llama_30B_SFT </a>  | 
| **Vietnamese-BLOOMZ v1.0 7B 2K Context Length**|ü§ó <a href="https://huggingface.co/VietnamAIHub/Vietnamese_bloomz_7b" target="_blank">Vietnamese_bloomz_7b </a>  | 

## Data

Here are some quick links to the datasets that we used to train the models:
| **Dataset**                      | **Link**                                                                                                                        | **Note**                    |
|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|
| **200K Mix Instruction-tuning**  |  [Vietnamese200k Mix Instructions](link_to_your_dataset)                                                                         |                              |
| **Vietnamese 52K Alpaca**        | [Vietnamese Alpaca 52k](https://github.com/VietnamAIHub/Vietnamese_LLMs/tree/main/Generate_and_Translate_Dataset/Vietnamese_Instructions_datasets/Translation/Alpaca_52k) | Translated using GPT-3.5    |
| **Vietnamese Lima 1K**           | [Vietnamese Lima 1K](https://github.com/VietnamAIHub/Vietnamese_LLMs/tree/main/Generate_and_Translate_Dataset/Vietnamese_Instructions_datasets/Translation/Lima/GPT_4_results)  | Translated by GPT-4         |
| **Vietnamese Dolly**             | [Vietnamese Dolly](link_to_your_dolly_dataset)                                                                                  | Translated by GPT-4         |
| **Vietnamese Instruction of How**             | [Vietnamese Instruction How Step by Step](https://huggingface.co/datasets/VietnamAIHub/Vietnamese_Instruction_How_Step_by_Step)                                                                                  | Extracted from Vietnamese WikiHow       |


# Demo: 
+ [**Vietnamese llama2 13B Model Demo**](https://bf930a52b9d6266882.gradio.live/)
+ [**Vietnamese llama2 7B Model Demo**](https://31fee86ed135939f28.gradio.live/)



<div align="center">
     <img width="auto" height="500px" src="./images/Vietassistant_GPT.gif"/>
</div>


# N·ªôi Dung (Table of Contents)

- [Gi·ªõi thi·ªáu v·ªÅ Vietnamese_LLMs](#Gi·ªõi-thi·ªáu-d·ª±-√°n)
- [M·ª•c ti√™u d·ª± √°n](#c√°c-li√™n-k·∫øt-h·ªØu-√≠ch)
<!-- - [C√°ch ti·∫øn h√†nh d·ª± √°n](#c√°ch-th·ª≠-nghi·ªám-open-assistant) -->
- [C·∫•u tr√∫c c·ªßa d·ª± √°n](#C·∫•u-tr√∫c-c·ªßa-d·ª±-√°n)
- [T·∫ßm nh√¨n](#t·∫ßm-nh√¨n)
- [K·∫ø Ho·∫°ch](#k·∫ø-ho·∫°ch)
- [L√†m th·∫ø n√†o b·∫°n c√≥ th·ªÉ gi√∫p ƒë·ª°](#l√†m-th·∫ø-n√†o-b·∫°n-c√≥-th·ªÉ-gi√∫p-ƒë·ª°)

## Gi·ªõi thi·ªáu d·ª± √°n (Project Introduction):

Ch√†o b·∫°n ƒë·∫øn v·ªõi d·ª± √°n C·ªông ƒë·ªìng LLMs Vi·ªát Nam! D·ª± √°n v·ªõi m·ª•c ti√™u t·∫°o ra b·ªô d·ªØ li·ªáu Vietnamese instruction v√†  th·ª±c hi·ªán Supervised instruction fine-tuning tr√™n c√°c Open-source m√¥ h√¨nh ng√¥n ng·ªØ  Bloom, OpenLLaMA, GPT-J, MPT, Pythia v√† nhi·ªÅu m√¥ h√¨nh kh√°c.

+ [D·ª± √°n T·ªïng Quan] ()

## M·ª•c ti√™u d·ª± √°n (Project Goal):

- X√¢y d·ª±ng B·ªô d·ªØ li·ªáu H∆∞·ªõng d·∫´n ti·∫øng Vi·ªát ch·∫•t l∆∞·ª£ng cao
- Hu·∫•n luy·ªán, Tinh ch·ªânh v√† ƒê√°nh gi√° M√¥ h√¨nh Ng√¥n ng·ªØ ti·∫øng Vi·ªát (Training, Finetuning, Evaluation)
- Thi·∫øt k·∫ø ·ª®ng d·ª•ng v·ªõi Giao di·ªán Ng∆∞·ªùi d√πng t·ªëi ∆∞u hi·ªáu su·∫•t

<!-- 
## C√°c nhi·ªám v·ª• (Tasks):

1. X√¢y d·ª±ng B·ªô d·ªØ li·ªáu Ti·∫øng Vi·ªát cho H∆∞·ªõng d·∫´n (Instructions) (ch·∫•t l∆∞·ª£ng, phong ph√∫ v√† ƒëa d·∫°ng):
   - Chuy·ªÉn ƒë·ªïi c√°c b·ªô d·ªØ li·ªáu H∆∞·ªõng d·∫´n Ti·∫øng Anh sang Ti·∫øng Vi·ªát.
   - T·ªïng h·ª£p c√°c ngu·ªìn d·ªØ li·ªáu ƒëa d·∫°ng c√≥ s·∫µn:
     + S·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu H∆∞·ªõng d·∫´n Ti·∫øng Vi·ªát t·ª´ wikiHow, v√≠ d·ª•: [human-instruction Vietnamese dataset](https://www.kaggle.com/datasets/paolop/human-instructions-vietnamese-wikihow?resource=download).
     + S·ª≠ d·ª•ng c√°c b·ªô d·ªØ li·ªáu t·ª´ lƒ©nh v·ª±c B√°o ch√≠, Y h·ªçc, Gi√°o d·ª•c, v.v., v√≠ d·ª•: b·ªô d·ªØ li·ªáu t·ª´ B√°o Corpus ([news-corpus](https://github.com/binhvq/news-corpus)).
   - T·∫°o b·ªï sung b·ªô d·ªØ li·ªáu t·ª± h·ªçc (self-instruct):
     + S·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª± h·ªçc nh∆∞ Stanford Alpaca.
     + T·∫°o b·ªô d·ªØ li·ªáu d·ª±a tr√™n c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn nh∆∞ GPT-3, GPT-3.5, GPT-4, PALM2, v.v.

2. Hu·∫•n luy·ªán v√† ƒê√°nh gi√° M√¥ h√¨nh Ng√¥n ng·ªØ (Training, Finetuning, Evaluating, Testing LLM):
   - Tinh ch·ªânh (Finetuning) c√°c m√¥ h√¨nh ng√¥n ng·ªØ m√£ ngu·ªìn m·ªü nh∆∞ bloomz, OpenLLaMA, GPT-J pythia, v.v. tr√™n B·ªô d·ªØ li·ªáu H∆∞·ªõng d·∫´n Ti·∫øng Vi·ªát.
     + √Åp d·ª•ng c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a (Compression Machine learning) nh∆∞ [Quantization](https://github.com/IST-DASLab/gptq), [Sparsity & Quantization](https://github.com/Vahe1994/SpQR).
     + S·ª≠ d·ª•ng k·ªπ thu·∫≠t tinh ch·ªânh hi·ªáu qu·∫£ nh∆∞ [LoRA]() v√† [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
     + √Åp d·ª•ng c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a Hu·∫•n luy·ªán v√† Tinh ch·ªânh nh∆∞ [Deepspeed](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/), [Colossal AI](https://colossalai.org/).
  - B·∫°n c√≥ th·ªÉ theo d√µi chi ti·∫øt model Finetuning 
  - ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh tr√™n c√°c b√†i ki·ªÉm tra (Benchmark) v√† c√°c t√¨nh hu·ªëng th·ª±c t·∫ø.
   - Ki·ªÉm th·ª≠ m√¥ h√¨nh tr√™n nhi·ªÅu c√°ch s·ª≠ d·ª•ng kh√°c nhau.

3. Thi·∫øt k·∫ø ·ª®ng d·ª•ng:
   - Thi·∫øt k·∫ø Giao di·ªán Ng∆∞·ªùi d√πng (UI) th√¢n thi·ªán v√† d·ªÖ s·ª≠ d·ª•ng.
   - T·ªëi ∆∞u hi·ªáu su·∫•t ·ª©ng d·ª•ng. -->

## C·∫•u Tr√∫c C·ªßa D·ª± √Ån (Project Structure)

D∆∞·ªõi ƒë√¢y l√† c·∫•u tr√∫c c·ªßa d·ª± √°n, m√¥ t·∫£ c√°c ph·∫ßn quan tr·ªçng v√† ch·ª©c nƒÉng ch√≠nh c·ªßa ch√∫ng:

### 1. T·∫°o v√† D·ªãch C√°c B·ªô D·ªØ li·ªáu (Generate and Translate Dataset)

Th∆∞ m·ª•c `/Generate_and_Translate_Dataset` ch·ª©a c√°c b·ªô d·ªØ li·ªáu v√† c√¥ng c·ª• li√™n quan ƒë·∫øn vi·ªác t·∫°o v√† d·ªãch c√°c instruction dataset.

- Ph·∫ßn D·ªãch (Translation Dataset)

  - `Using_OpenAI_Translate_API.py`: S·ª≠ d·ª•ng OpenAI GPT-3.5 v√† GPT-4 ƒë·ªÉ d·ªãch c√°c b·ªô d·ªØ li·ªáu. ƒê√¢y l√† m·ªôt ph∆∞∆°ng ph√°p cho k·∫øt qu·∫£ t·ªët.

  - `Using_NLLB_MetaAI_Translate.py`: S·ª≠ d·ª•ng NLLB l√†m m√¥ h√¨nh cho vi·ªác d·ªãch. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng 54B model ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªëi.

- Ph·∫ßn T·∫°o Instruction Dataset 

  - Chi ti·∫øt k·ªπ thu·∫≠t d√πng [t·∫°o Instruction dataset](https://docs.google.com/presentation/d/1qfIQoGMmarlZWzRa5lVQrMD67SmoVb7F6jr5NS0_Hx0/edit#slide=id.g22944aa9b74_2_399) t·ª´ Slide 8 t·ªõi slide 14

  - `Generation_instruction_OpenAI_api.py`: S·ª≠ d·ª•ng Stanford Alpaca template ƒë·ªÉ t·∫°o c√°c instruction dataset. G·ªìm h∆°n 175 instruction tasks ƒë∆∞·ª£c t·∫°o b·ªüi con ng∆∞·ªùi.

  - C·∫≠p Nh·∫≠p S·ªõm trong T∆∞∆°ng Lai: Ph·∫ßn n√†y d·ª± ki·∫øn s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t v·ªõi th√¥ng tin v·ªÅ c√°ch t·∫°o th√™m Instruction dataset t·ª´ c√°c ngu·ªìn kh√°c.

### 2. Training & Fine-tune LLM Model

Th∆∞ m·ª•c `/LLMs` ch·ª©a c√°c t·ªáp tin v√† c√¥ng c·ª• ƒë·ªÉ training v√† fine-tune c√°c m√¥ h√¨nh ng√¥n ng·ªØ (Language Models).

- Ph·∫ßn Fine-tuning d·ª±a tr√™n c√°c Open-Source Based LLMs (BLOOMZ, Open-LLaMA, v.v.)

  - `Finetune_llm_LoRA.py`: Cung c·∫•p c√¥ng c·ª• ƒë·ªÉ fine-tune c√°c m√¥ h√¨nh LLMs d·ª±a tr√™n c√°c m√£ ngu·ªìn m·ªü nh∆∞ BLOOMZ, Open-LLaMA, v.v.

  - `Finetune_llm_QLoRA.py`: ƒê√¢y l√† m·ªôt c√¥ng c·ª• kh√°c ƒë·ªÉ fine-tune c√°c m√¥ h√¨nh LLMs d·ª±a tr√™n c√°c m√£ ngu·ªìn m·ªü.

### 3. Giao Di·ªán Web (Web UI Interface)

Th∆∞ m·ª•c `/WebUI` ch·ª©a c√°c t·ªáp tin v√† c√¥ng c·ª• li√™n quan ƒë·∫øn giao di·ªán ng∆∞·ªùi d√πng qua Web.

- Hi·ªán t·∫°i, ƒë·ªÉ nhanh ch√≥ng v√† thu·∫≠n ti·ªán cho vi·ªác demo v√† ki·ªÉm th·ª≠, ch√∫ng t√¥i s·ª≠ d·ª•ng Gradio ƒë·ªÉ ph√°t tri·ªÉn giao di·ªán.

  - `assistant_gradio.py`: ƒê√¢y l√† ·ª©ng d·ª•ng ƒë√£ ƒë∆∞·ª£c ph√°t tri·ªÉn d·ª±a tr√™n Gradio, cho ph√©p tr·∫£i nghi·ªám tr·ª±c quan v√† tr√≤ chuy·ªán v·ªõi tr·ª£ l√Ω th√¥ng qua giao di·ªán Web.

Hy v·ªçng V·ªõi c·∫•u tr√∫c n√†y, d·ª± √°n c√≥ th·ªÉ ƒë∆∞·ª£c qu·∫£n l√Ω m·ªôt c√°ch c·ª• th·ªÉ v√† d·ªÖ ƒë√†ng ƒë·ªÉ c·∫≠p nh·∫≠p [m·ªçi ng∆∞·ªùi c√≥ th·ªÉ g√≥p √Ω ƒë·ªÉ c√≥ m·ªôt c·∫•u tr√∫c t·ªët h∆°n]()

## T·∫ßm Nh√¨n (Project Vision)

[Chi Ti·∫øt v·ªÅ Vision & Roadmap](https://docs.google.com/presentation/d/1qfIQoGMmarlZWzRa5lVQrMD67SmoVb7F6jr5NS0_Hx0/edit?usp=sharing)

X√¢y d·ª±ng tr·ª£ l√Ω th√¥ng minh ti·∫øng Vi·ªát c·ªßa t∆∞∆°ng lai, v∆∞·ª£t tr·ªôi v√† linh ho·∫°t h∆°n bao gi·ªù h·∫øt!

+ Ch√∫ng ta s·∫Ω t·∫°o ra m·ªôt m√¥ h√¨nh LLMs (Language Models) ti√™n ti·∫øn c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω t·ªët c√°c t√°c v·ª• ti·∫øng Vi·ªát. M·ª•c ti√™u c·ªßa ph√°t tri·ªÉn m·∫°nh c√°c LLMs  ·ª©ng d·ª•ng kh√°c trong Gi√°o d·ª•c, Y t·∫ø, T√†i ch√≠nh v√† C√¥ng nghi·ªáp.

+ ƒêi·ªÅu ƒë·∫∑c bi·ªát l√† ch√∫ng ta mu·ªën t·∫°o ra m√¥ h√¨nh tr·ª£ l√Ω c√≥ kh·∫£ nƒÉng t∆∞∆°ng t√°c n√¢ng cao tr√™n ti·∫øng Vi·ªát. H∆°n th·∫ø n·ªØa ch√∫ng ta c≈©ng ƒë·∫∑t m·ª•c ti√™u l√†m cho m√¥ h√¨nh n√†y nh·ªè g·ªçn v√† hi·ªáu qu·∫£, ƒë·ªÉ c√≥ th·ªÉ ch·∫°y tr√™n c√°c ng∆∞·ªùi d√πng c√° nh√¢n m√°y t√≠nh v·ªõi c√°c GPUs th·∫ø h·ªá th·∫•p v·ªõi √≠t memory.

+ D·ª± √°n n√†y ch√∫ng ta mong mu·ªën nh·∫≠n ƒë∆∞·ª£c s·ª± ƒë√≥ng g√≥p v√† h·ªó tr·ª£ c·ªông ƒë·ªìng. H√£y c√πng nhau x√¢y d·ª±ng m·ªôt tr·ª£ l√Ω th√¥ng minh n√≥i ri√™ng v√† m√¥ h√¨nh ng√¥n ng·ªØa thu·∫ßn ti·∫øng Vi·ªát n√≥i chung v√† g√≥p ph·∫ßn t·∫°o ra nh·ªØng ƒë√≥ng g√≥p √Ω nghƒ©a cho c·ªông ƒë·ªìng Vi·ªát Nam üáªüá≥.

## K·∫ø Ho·∫°ch (Project plan)

<!--  [C·∫•u tr√∫c c·ªßa d·ª± √°n](https://docs.google.com/presentation/d/1OdCTI1vMpftOMTOXXHEt2Ck5SBLSkPf_Zwedq7n3wec/edit?usp=sharing) -->

### B∆∞·ªõc 1: D·ªãch t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n
- M·ª•c ti√™u: D·ªãch c√°c b·ªô d·ªØ li·ªáu chu·∫©n v√† ch·∫•t L∆∞·ª£ng English based instructions dataset : [Alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json), [Dolly 15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1), [Filtered_ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) others dataset.
- X√¢y d·ª±ng h·ªá th·ªëng, th·ªëng k√™ hi·ªÉn th·ªã c√°c ch·ªß ƒë·ªÅ kh√°c nhau trong t·∫≠p d·ªØ li·ªáu ƒë√£ thu th·∫≠p. M·ª•c ƒë√≠ch l√† lo·∫°i b·ªè d·ªØ li·ªáu ch·ª©a th√¥ng tin g√¢y l·∫∑n, ƒë·ªôc h·∫°i, spam, r√°c r∆∞·ªüi ho·∫∑c th√¥ng tin c√° nh√¢n ho·∫∑c c√°c d·ªØ kh√¥ng ƒë·∫°t y√™u c·∫ßu.

### B∆∞·ªõc 2: T·∫°o t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n t·ª± ƒë·ªông
- S·ª≠ d·ª•ng OpenAI GPT-3.5, GPT-4 ƒë·ªÉ t·∫°o t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n.
- M·ª•c ti√™u: Thu th·∫≠p 500.000 ƒë·∫øn 1 tri·ªáu m·∫´u h∆∞·ªõng d·∫´n ƒë·∫ßu v√†o + ph·∫£n h·ªìi (Instructions, outputs)
- ƒê·ªìng th·ªùi, ch√∫ng t√¥i thu th·∫≠p c√°c h∆∞·ªõng d·∫´n ƒë∆∞·ª£c t·∫°o b·ªüi con ng∆∞·ªùi c√≥ s·∫µn b·∫±ng ti·∫øng Vi·ªát.

### B∆∞·ªõc 3: Ki·ªÉm ƒë·ªãnh v√† ti·ªÅn x·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu
- K·∫øt h·ª£p t·∫≠p d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1 v√† B∆∞·ªõc 2.
- Ti·ªÅn x·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu ƒë·ªÉ chu·∫©n b·ªã cho c√°c b∆∞·ªõc ti·∫øp theo.

### B∆∞·ªõc 4: Ti·∫øn h√†nh giai ƒëo·∫°n SFT (Supervised instruction Finetuning)
- D·ª±a tr√™n t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n ti·∫øng Vi·ªát, ti·∫øn h√†nh giai ƒëo·∫°n SFT ƒë·ªÉ tinh ch·ªânh m√¥ h√¨nh.

### B∆∞·ªõc 5: Ti·∫øp t·ª•c hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi giai ƒëo·∫°n RLHF (Reinforcement Learning from Human Feedback)
- Sau khi ho√†n th√†nh B∆∞·ªõc 4, ch√∫ng ta c√≥ th·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi giai ƒëo·∫°n RLHF d·ª±a tr√™n t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n t·ª´ con ng∆∞·ªùi thu·ªôc d·ª± √°n OpenAssistant c√¥ng khai.

H√£y nh·ªõ r·∫±ng c√°c b∆∞·ªõc n√†y ƒë·∫°i di·ªán cho quy tr√¨nh chung v√† c√≥ th·ªÉ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh v√† b·ªï sung theo y√™u c·∫ßu c·ª• th·ªÉ c·ªßa d·ª± √°n.

## L√†m Th·∫ø N√†o B·∫°n C√≥ Gi√∫p ƒê·ª° (How You can HELP)

Ch√∫ng ta c√≥ th·ªÉ c√πng nhau ƒë√≥ng g√≥p tri th·ª©c v√† c√¥ng ngh·ªá c·ªßa m√¨nh ƒë·ªÉ mang l·∫°i l·ª£i √≠ch cho c·ªông ƒë·ªìng Vi·ªát Nam.

1. b·∫°n c√≥ th·ªÉ c√πng x√¢y d·ª±ng d·ª± √°n: 
H√£y xem h∆∞·ªõng d·∫´n [ƒê√≥ng G√≥p Cho D·ª± √Ån](contribute.md) ƒë·ªÉ b·∫Øt ƒë·∫ßu chung tay x√¢y d·ª±ng d·ª± √°n n√†y.

2. B·∫°n c√≥ th·ªÉ h·ªï tr·ª£ v·ªÅ t√†i nguy√™n nh∆∞ m√°y ch·ªß server ho·∫∑c c√°c t√†i nguy√™n kh√°c.
  - D·ª± √°n hi·ªán r·∫•t c·∫ßn c√°c ngu·ªìn t√†i tr·ª£ t√†i nguy√™n GPUs ƒë·ªÉ c√≥ th·ªÉ ti·∫øn h√†nh qu√° tr√¨nh hu·∫•n luy·ªán (Pretraining) v√† qu√° tr√¨nh tinh ch·ªânh (Finetuning).
  - N·∫øu b·∫°n C√≥ th·ªÉ gi√∫p d·ª± √°n k·∫øt n·ªëi v·ªõi c√°c c√¥ng ty t∆∞ nh√¢n ƒë·ªÉ mang d·ª± √°n n√†y √°p d·ª•ng r·ªông r√£i.
  - B·∫°n c√≥ th·ªÉ k·∫øt n·ªëi tr·ª±c ti·∫øp v·ªõi Tr·∫ßn Nhi·ªám [LinkedIn](https://www.linkedin.com/in/tran-nhiem-ab1851125/) [Facebook](https://www.facebook.com/jean.tran.336). email: tvnhiemhcmus@gmail.com .

```
@misc{vietnameseLLM,
    author={Tran Nhiem},
    title={Vietnamese Instruction Data Corpus for Large-Scale Finetuning of Language Models},
    year={2023},
    url={https://github.com/VietnamAIHub/Vietnamese_LLMs},
}
```
