
<h1 align="center">
  <span> üáªüá≥ C·ªông ƒë·ªìng LLMs Vi·ªát Nam - Vietnamese Language Models Community</span>
</h1>

<div align="center">
     <img width="auto" height="400px" src="./images/Vietnamese_LLMs_pipeline.png"/>
</div>

## üí° Get help - [Q&A](https://github.com/TranNhiem/Vietnamese_LLMs/discussions) or [Discord üí¨](https://discord.gg/eH7eg4fT)

# N·ªôi Dung (Table of Contents)

- [Gi·ªõi thi·ªáu v·ªÅ Vietnamese_LLMs](#Gi·ªõi-thi·ªáu-d·ª±-√°n)
- [M·ª•c ti√™u d·ª± √°n](#c√°c-li√™n-k·∫øt-h·ªØu-√≠ch)
- [C√°ch ti·∫øn h√†nh d·ª± √°n](#c√°ch-th·ª≠-nghi·ªám-open-assistant)
- [T·∫ßm nh√¨n](#t·∫ßm-nh√¨n)
- [K·∫ø Ho·∫°ch](#k·∫ø-ho·∫°ch)
- [L√†m th·∫ø n√†o b·∫°n c√≥ th·ªÉ gi√∫p ƒë·ª°](#l√†m-th·∫ø-n√†o-b·∫°n-c√≥-th·ªÉ-gi√∫p-ƒë·ª°)

## Gi·ªõi thi·ªáu d·ª± √°n (Project Introduction):

Ch√†o b·∫°n ƒë·∫øn v·ªõi d·ª± √°n C·ªông ƒë·ªìng LLMs Vi·ªát Nam! D·ª± √°n v·ªõi m·ª•c ti√™u t·∫°o ra b·ªô d·ªØ li·ªáu Vietnamese instruction v√†  th·ª±c hi·ªán Supervised instruction fine-tuning tr√™n c√°c Open-source m√¥ h√¨nh ng√¥n ng·ªØ  Bloom, OpenLLaMA, GPT-J pythia v√† nhi·ªÅu m√¥ h√¨nh kh√°c.


## M·ª•c ti√™u d·ª± √°n (Project Goal):

- X√¢y d·ª±ng B·ªô d·ªØ li·ªáu H∆∞·ªõng d·∫´n ti·∫øng Vi·ªát ch·∫•t l∆∞·ª£ng cao
- Hu·∫•n luy·ªán, Tinh ch·ªânh v√† ƒê√°nh gi√° M√¥ h√¨nh Ng√¥n ng·ªØ ti·∫øng Vi·ªát (Training, Finetuning, Evaluation)
- Thi·∫øt k·∫ø ·ª®ng d·ª•ng v·ªõi Giao di·ªán Ng∆∞·ªùi d√πng t·ªëi ∆∞u hi·ªáu su·∫•t

## C√°c nhi·ªám v·ª• (Tasks):

1. X√¢y d·ª±ng B·ªô d·ªØ li·ªáu ti·∫øng Vi·ªát Instructions Vietnamese (ch·∫•t l∆∞·ª£ng, nhi·ªÅu, v√† ƒëa d·∫°ng):
   - D·ªãch c√°c b·ªô d·ªØ li·ªáu ti·∫øng Anh
   - T·ªïng h·ª£p ngu·ªìn d·ªØ li·ªáu ƒëa d·∫°ng
   + B·ªô d·ªØ li·ªáu v·ªÅ B√°o Ch√≠ 
   + 
   - T·∫°o d·ªØ li·ªáu t·ª± h·ªçc b·ªï sung

   + Reference Consider Project Dataset and Design How to deploy this to this Project
   
   - T·∫°o B·ªô d·ªØ li·ªáu d·ª±a tr√™n c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (GPT3, GPT-3.5, GPT-4, PALM2 etc)

2. Hu·∫•n luy·ªán v√† ƒê√°nh gi√° M√¥ h√¨nh Ng√¥n ng·ªØ (Training, Finetuning, Evaluating, Testing LLM):
   - Finetuning (Tinh ch·ªânh) c√°c Open-source LLMs m√¥ h√¨nh ng√¥n ng·ªØ : bloomz, OpenLLaMA, GPT-J pythia etc. tr√™n Vietnamese Instruction Dataset
   - ƒê√°nh gi√° hi·ªáu su·∫•t

3. Thi·∫øt k·∫ø ·ª®ng d·ª•ng 
   - Giao di·ªán Ng∆∞·ªùi d√πng (UI)
   - T·ªëi ∆∞u hi·ªáu su·∫•t ·ª©ng d·ª•ng

## T·∫ßm Nh√¨n (Project Vision)
[Vision & Roadmap](https://docs.google.com/presentation/d/1qfIQoGMmarlZWzRa5lVQrMD67SmoVb7F6jr5NS0_Hx0/edit?usp=sharing)

+ Slide 
+ 
We are not going to stop at replicating ChatGPT. We want to build the assistant
of the future, able to not only write email and cover letters, but do meaningful
work, use APIs, dynamically research information, and much more, with the
ability to be personalized and extended by anyone. And we want to do this in a
way that is open and accessible, which means we must not only build a great
assistant, but also make it small and efficient enough to run on consumer
hardware.

+ RLHF
+ AgentLLM
+ Application LLMs on (Education, Medical, Finance, Industries)

## K·∫ø Ho·∫°ch (Project plan)

[C·∫•u tr√∫c c·ªßa d·ª± √°n] (https://docs.google.com/presentation/d/1OdCTI1vMpftOMTOXXHEt2Ck5SBLSkPf_Zwedq7n3wec/edit?usp=sharing)


1. Collect high-quality human generated Instruction-Fulfillment samples
   (prompt + response), goal >50k. We design a crowdsourced process to collect
   and reviewed prompts. We do not want to train on
   flooding/toxic/spam/junk/personal information data. We will have a
   leaderboard to motivate the community that shows progress and the most active
   users. Swag will be given to the top-contributors.
2. For each of the collected prompts we will sample multiple completions.
   Completions of one prompt will then be shown randomly to users to rank them
   from best to worst. Again this should happen crowd-sourced, e.g. we need to
   deal with unreliable potentially malicious users. At least multiple votes by
   independent users have to be collected to measure the overall agreement. The
   gathered ranking-data will be used to train a reward model.
3. Now follows the RLHF training phase based on the prompts and the reward
   model.

We can then take the resulting model and continue with completion sampling step
2 for a next iteration.

## L√†m Th·∫ø N√†o B·∫°n C√≥ Gi√∫p ƒê·ªü (How You can HELP)

we collaborate we can together gift our knowledge and technology to the world for the benefit of humanity.
1. how to Build Project Together
Check out our  [contributing guide](contribute.md) to get started. 

2. We still Need more Computing Resources
  + Please help us sponser Traning Compute 
  + Please help us to connect private sectors in public to bring this project at scale.

